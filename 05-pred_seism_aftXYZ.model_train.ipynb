{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Model training step for the project '**Aftershock pattern prediction based on earthquake rupture data for improved seismic hazard assessment**' (pred_seism_aftXYZ). DeVries18 will refer to the article 'Deep learning of aftershock patterns following large earthquakes' by Phoebe M. R. DeVries, Fernanda Vi√©gas, Martin Wattenberg & Brendan J. Meade, and published in Nature in 2018 (https://www.nature.com/articles/s41586-018-0438-y ).\n",
    "\n",
    "Inputs from previous steps of the process model:\n",
    "-  'Features.pkl': Features and target of baseline model (DeVries18), as pickle file;\n",
    "-  'new_features.pkl': New features and same target as baseline, as pickle file;\n",
    "-  'model_baseline_prev.h5': Baseline deep neural net of DeVries18, as (untrained) Keras model HDF5 file;\n",
    "-  'model_baseline_DeVries18_simplified_init.h5': Simplified topology for baseline DNN of DeVries18, as (untrained) Keras model HDF5 file;\n",
    "-  'model_DNN_init.h5': Deep neural net topology proposed for this project, as (untrained) Keras model HDF5 file;\n",
    "-  'model_ANN_init.h5': Shallow artificial neural net proposed for this project, as (untrained) Keras model HDF5 file.\n",
    "\n",
    "For comparison with the DeVries study, we will use the same training and test sets ('Training_FileNames.h5', 'Testing_FileNames.h5'), first imported from the [Google Drive](https://drive.google.com/drive/folders/1c5Rb_6EsuP2XedDjg37bFDyf8AadtGDa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "h5file1 = h5py.File('./Data/Training_FileNames.h5', 'r')\n",
    "training_filenames = np.array(h5file1.get('file_names_training'))\n",
    "h5file2 = h5py.File('./Data/Testing_FileNames.h5', 'r')\n",
    "testing_filenames = np.array(h5file2.get('file_names_testing'))\n",
    "\n",
    "training_IDs_temp = map(lambda x: str(x, 'utf-8'), training_filenames)  \n",
    "training_IDs = list(map(lambda x: 's' + x[0:x.find('_')], training_IDs_temp))\n",
    "testing_IDs_temp = map(lambda x: str(x, 'utf-8'), testing_filenames)   \n",
    "testing_IDs = list(map(lambda x: 's' + x[0:x.find('_')], testing_IDs_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.read_pickle('./Data/Features.pkl')\n",
    "new_features = pd.read_pickle('./Data/new_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[164177, 38467, 0.8101744931998973]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training & testing sets\n",
    "TrainingSet_prev = Features.loc[Features['ID'].isin(training_IDs)]\n",
    "TestingSet_prev = Features.loc[Features['ID'].isin(testing_IDs)]\n",
    "TrainingSet_new = new_features.loc[new_features['ID'].isin(training_IDs)]\n",
    "TestingSet_new = new_features.loc[new_features['ID'].isin(testing_IDs)]\n",
    "\n",
    "# ratio of training samples\n",
    "[len(TrainingSet_prev), len(TestingSet_prev), len(TrainingSet_prev)/(len(TrainingSet_prev)+len(TestingSet_prev))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training sets\n",
    "TrainingSet_prev = sklearn.utils.shuffle(TrainingSet_prev)\n",
    "TrainingSet_new = sklearn.utils.shuffle(TrainingSet_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>aftershocksyn</th>\n",
       "      <th>posabsxx</th>\n",
       "      <th>posabsxy</th>\n",
       "      <th>posabsyy</th>\n",
       "      <th>posabsxz</th>\n",
       "      <th>posabsyz</th>\n",
       "      <th>posabszz</th>\n",
       "      <th>negabsxx</th>\n",
       "      <th>negabsxy</th>\n",
       "      <th>negabsyy</th>\n",
       "      <th>negabsxz</th>\n",
       "      <th>negabsyz</th>\n",
       "      <th>negabszz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28144</th>\n",
       "      <td>s1968TOKACH01NAGA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054881</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>-0.054881</td>\n",
       "      <td>-0.024984</td>\n",
       "      <td>-0.010592</td>\n",
       "      <td>-0.007663</td>\n",
       "      <td>-0.003465</td>\n",
       "      <td>-0.009011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93258</th>\n",
       "      <td>s2004SUMATR01AMMO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019217</td>\n",
       "      <td>0.091083</td>\n",
       "      <td>0.202546</td>\n",
       "      <td>0.072640</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>-0.019217</td>\n",
       "      <td>-0.091083</td>\n",
       "      <td>-0.202546</td>\n",
       "      <td>-0.072640</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>-0.023352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72370</th>\n",
       "      <td>s2011TOHOKU01SATA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.339550</td>\n",
       "      <td>0.334037</td>\n",
       "      <td>0.813508</td>\n",
       "      <td>0.514822</td>\n",
       "      <td>0.051104</td>\n",
       "      <td>0.581820</td>\n",
       "      <td>-0.339550</td>\n",
       "      <td>-0.334037</td>\n",
       "      <td>-0.813508</td>\n",
       "      <td>-0.514822</td>\n",
       "      <td>-0.051104</td>\n",
       "      <td>-0.581820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>s2010MAULEC01HAYE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.847396</td>\n",
       "      <td>0.205944</td>\n",
       "      <td>1.139442</td>\n",
       "      <td>0.498965</td>\n",
       "      <td>0.613154</td>\n",
       "      <td>0.200377</td>\n",
       "      <td>-1.847396</td>\n",
       "      <td>-0.205944</td>\n",
       "      <td>-1.139442</td>\n",
       "      <td>-0.498965</td>\n",
       "      <td>-0.613154</td>\n",
       "      <td>-0.200377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72234</th>\n",
       "      <td>s2011TOHOKU01SATA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596709</td>\n",
       "      <td>0.283883</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.300852</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.034331</td>\n",
       "      <td>-0.596709</td>\n",
       "      <td>-0.283883</td>\n",
       "      <td>-0.231554</td>\n",
       "      <td>-0.300852</td>\n",
       "      <td>-0.002066</td>\n",
       "      <td>-0.034331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53030</th>\n",
       "      <td>s2004SUMATR02RHIE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.177236</td>\n",
       "      <td>0.324688</td>\n",
       "      <td>0.544993</td>\n",
       "      <td>0.163446</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>-1.177236</td>\n",
       "      <td>-0.324688</td>\n",
       "      <td>-0.544993</td>\n",
       "      <td>-0.163446</td>\n",
       "      <td>-0.074585</td>\n",
       "      <td>-0.007162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28253</th>\n",
       "      <td>s2011TOHOKU02FUJI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.175265</td>\n",
       "      <td>0.007689</td>\n",
       "      <td>0.631467</td>\n",
       "      <td>0.891350</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.618322</td>\n",
       "      <td>-1.175265</td>\n",
       "      <td>-0.007689</td>\n",
       "      <td>-0.631467</td>\n",
       "      <td>-0.891350</td>\n",
       "      <td>-0.668571</td>\n",
       "      <td>-0.618322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161988</th>\n",
       "      <td>s2004SUMATR02RHIE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>3.431447</td>\n",
       "      <td>0.638794</td>\n",
       "      <td>0.068985</td>\n",
       "      <td>2.289272</td>\n",
       "      <td>2.090269</td>\n",
       "      <td>-0.557100</td>\n",
       "      <td>-3.431447</td>\n",
       "      <td>-0.638794</td>\n",
       "      <td>-0.068985</td>\n",
       "      <td>-2.289272</td>\n",
       "      <td>-2.090269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49494</th>\n",
       "      <td>s2011TOHOKU01SATA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505163</td>\n",
       "      <td>0.412470</td>\n",
       "      <td>0.305224</td>\n",
       "      <td>0.126985</td>\n",
       "      <td>0.099218</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.505163</td>\n",
       "      <td>-0.412470</td>\n",
       "      <td>-0.305224</td>\n",
       "      <td>-0.126985</td>\n",
       "      <td>-0.099218</td>\n",
       "      <td>-0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58651</th>\n",
       "      <td>s2011TOHOKU01SATA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.073595</td>\n",
       "      <td>0.063582</td>\n",
       "      <td>0.033686</td>\n",
       "      <td>0.288990</td>\n",
       "      <td>0.082774</td>\n",
       "      <td>0.055018</td>\n",
       "      <td>-0.073595</td>\n",
       "      <td>-0.063582</td>\n",
       "      <td>-0.033686</td>\n",
       "      <td>-0.288990</td>\n",
       "      <td>-0.082774</td>\n",
       "      <td>-0.055018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID  aftershocksyn  posabsxx  posabsxy  posabsyy  \\\n",
       "28144   s1968TOKACH01NAGA            0.0  0.054881  0.024984  0.010592   \n",
       "93258   s2004SUMATR01AMMO            1.0  0.019217  0.091083  0.202546   \n",
       "72370   s2011TOHOKU01SATA            1.0  0.339550  0.334037  0.813508   \n",
       "69997   s2010MAULEC01HAYE            1.0  1.847396  0.205944  1.139442   \n",
       "72234   s2011TOHOKU01SATA            0.0  0.596709  0.283883  0.231554   \n",
       "53030   s2004SUMATR02RHIE            1.0  1.177236  0.324688  0.544993   \n",
       "28253   s2011TOHOKU02FUJI            1.0  1.175265  0.007689  0.631467   \n",
       "161988  s2004SUMATR02RHIE            0.0  0.557100  3.431447  0.638794   \n",
       "49494   s2011TOHOKU01SATA            1.0  0.505163  0.412470  0.305224   \n",
       "58651   s2011TOHOKU01SATA            1.0  0.073595  0.063582  0.033686   \n",
       "\n",
       "        posabsxz  posabsyz  posabszz  negabsxx  negabsxy  negabsyy  negabsxz  \\\n",
       "28144   0.007663  0.003465  0.009011 -0.054881 -0.024984 -0.010592 -0.007663   \n",
       "93258   0.072640  0.021217  0.023352 -0.019217 -0.091083 -0.202546 -0.072640   \n",
       "72370   0.514822  0.051104  0.581820 -0.339550 -0.334037 -0.813508 -0.514822   \n",
       "69997   0.498965  0.613154  0.200377 -1.847396 -0.205944 -1.139442 -0.498965   \n",
       "72234   0.300852  0.002066  0.034331 -0.596709 -0.283883 -0.231554 -0.300852   \n",
       "53030   0.163446  0.074585  0.007162 -1.177236 -0.324688 -0.544993 -0.163446   \n",
       "28253   0.891350  0.668571  0.618322 -1.175265 -0.007689 -0.631467 -0.891350   \n",
       "161988  0.068985  2.289272  2.090269 -0.557100 -3.431447 -0.638794 -0.068985   \n",
       "49494   0.126985  0.099218  0.026316 -0.505163 -0.412470 -0.305224 -0.126985   \n",
       "58651   0.288990  0.082774  0.055018 -0.073595 -0.063582 -0.033686 -0.288990   \n",
       "\n",
       "        negabsyz  negabszz  \n",
       "28144  -0.003465 -0.009011  \n",
       "93258  -0.021217 -0.023352  \n",
       "72370  -0.051104 -0.581820  \n",
       "69997  -0.613154 -0.200377  \n",
       "72234  -0.002066 -0.034331  \n",
       "53030  -0.074585 -0.007162  \n",
       "28253  -0.668571 -0.618322  \n",
       "161988 -2.289272 -2.090269  \n",
       "49494  -0.099218 -0.026316  \n",
       "58651  -0.082774 -0.055018  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingSet_prev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['posabsxx', 'posabsxy', 'posabsyy', 'posabsxz', 'posabsyz', 'posabszz',\n",
    "           'negabsxx', 'negabsxy', 'negabsyy', 'negabsxz', 'negabsyz', 'negabszz']\n",
    "\n",
    "target = 'aftershocksyn'\n",
    "\n",
    "x_train = TrainingSet_prev[features]\n",
    "y_train = TrainingSet_prev[target]\n",
    "x_test = TestingSet_prev[features]\n",
    "y_test = TestingSet_prev[target]\n",
    "\n",
    "x_test.to_pickle(\"./Data/TestingSet_X_prev.pkl\")\n",
    "y_test.to_pickle(\"./Data/TestingSet_y_prev.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 4s 37ms/step - loss: 0.7467 - binary_accuracy: 0.5055 - val_loss: 0.7329 - val_binary_accuracy: 0.4976\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 0.7439 - binary_accuracy: 0.5113 - val_loss: 0.7316 - val_binary_accuracy: 0.4991\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 1s 30ms/step - loss: 0.7452 - binary_accuracy: 0.5129 - val_loss: 0.7303 - val_binary_accuracy: 0.4999\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 0.7445 - binary_accuracy: 0.5162 - val_loss: 0.7289 - val_binary_accuracy: 0.5013\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 0.7432 - binary_accuracy: 0.5169 - val_loss: 0.7276 - val_binary_accuracy: 0.5021\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# same as in DeVries18\n",
    "baselinemodel_prev = load_model('./Models/model_baseline_prev.h5')\n",
    "batch_size = 3500\n",
    "epochs = 5\n",
    "\n",
    "history = baselinemodel_prev.fit(x_train, y_train,\n",
    "                            batch_size = batch_size,\n",
    "                            epochs = epochs,\n",
    "                            validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the baseline model CV results - we should aim at achieving similar or greater accuracy ($\\geq$ 72%) for the new model with new set of features. This will however not assure a same or better generalization than the baseline model (AUC = 85%, see next step of the process model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselinemodel_prev.save('./Models/model_baseline_prev_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note\n",
    "\n",
    "Since the Devries18 model uses a relatively small input layer (12 nodes), a simpler DNN topology should do just fine. We show it, only for illustrative purpose. The model proposed in this project will use another set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 1s 9ms/step - loss: 0.6694 - binary_accuracy: 0.5168 - val_loss: 0.6645 - val_binary_accuracy: 0.5332\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.6692 - binary_accuracy: 0.5266 - val_loss: 0.6644 - val_binary_accuracy: 0.5384\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.6689 - binary_accuracy: 0.5330 - val_loss: 0.6643 - val_binary_accuracy: 0.5429\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.6691 - binary_accuracy: 0.5369 - val_loss: 0.6642 - val_binary_accuracy: 0.5471\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.6689 - binary_accuracy: 0.5425 - val_loss: 0.6641 - val_binary_accuracy: 0.5539\n"
     ]
    }
   ],
   "source": [
    "baselinemodel_DeVries18_simplified = load_model('./Models/model_baseline_DeVries18_simplified_init.h5')\n",
    "\n",
    "history = baselinemodel_DeVries18_simplified.fit(x_train, y_train,\n",
    "                            batch_size = batch_size,\n",
    "                            epochs = epochs,\n",
    "                            validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the similar CV accuracy using 12 - 8 - 8 - 1 instead of 12 - 50 - 50 - 50 - 50 - 50 - 50 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselinemodel_DeVries18_simplified.save('./Models/model_baseline_DeVries18_simplified_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new models\n",
    "\n",
    "Training new models with the proposed set of features based on geometry and kinematics (minimum distance to mainshock rupture and mean slip on rupture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = ['mindist', 'dipMean', 'strikeMean', 'slipMean']   #best 4 - 12 - 12 - 1\n",
    "features = ['mindist', 'slipMean']   #best 2 - 6 - 6 - 1\n",
    "target = 'aftershocksyn'\n",
    "\n",
    "x_train = TrainingSet_new[features]\n",
    "y_train = TrainingSet_new[target]\n",
    "x_test = TestingSet_new[features]\n",
    "y_test = TestingSet_new[target]\n",
    "\n",
    "# save for next step of the process model\n",
    "x_test.to_pickle(\"./Data/TestingSet_X_new.pkl\")\n",
    "y_test.to_pickle(\"./Data/TestingSet_y_new.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new DNN (simplified topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.6839 - binary_accuracy: 0.6402 - val_loss: 0.6831 - val_binary_accuracy: 0.7086\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6840 - binary_accuracy: 0.6432 - val_loss: 0.6830 - val_binary_accuracy: 0.7086\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6839 - binary_accuracy: 0.6441 - val_loss: 0.6830 - val_binary_accuracy: 0.7086\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6839 - binary_accuracy: 0.6448 - val_loss: 0.6830 - val_binary_accuracy: 0.7086\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6837 - binary_accuracy: 0.6443 - val_loss: 0.6830 - val_binary_accuracy: 0.7086\n"
     ]
    }
   ],
   "source": [
    "model_DNN = load_model('./Models/model_DNN_init.h5')\n",
    "\n",
    "history = model_DNN.fit(x_train, y_train,\n",
    "                            batch_size = batch_size,\n",
    "                            epochs = epochs,\n",
    "                            validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documenting a first DNN model result\n",
    "# we tried the following topology 4 - 12 - 12 - 1, the 4 features being: ['mindist', 'dipMean', 'strikeMean', 'slipMean']\n",
    "# we obtained: \n",
    "#Epoch 5/5\n",
    "#147801/147801 [==============================] - 0s 2us/step - loss: 0.6134 - binary_accuracy: 0.6829 - \n",
    "#                                                           val_loss: 0.5801 - val_binary_accuracy: 0.7228\n",
    "\n",
    "# additional tuning involved changes in topology, using relu instead of tanh activation, dropout rate change [NOT SHOWN for clarity]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DNN.save('./Models/model_DNN_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ANN\n",
    "\n",
    "Training of an Artificial Neural Network with one hidden layer (no deep learning!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 1s 17ms/step - loss: 0.7372 - binary_accuracy: 0.2931 - val_loss: 0.7363 - val_binary_accuracy: 0.3011\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7372 - binary_accuracy: 0.2931 - val_loss: 0.7362 - val_binary_accuracy: 0.3015\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7371 - binary_accuracy: 0.2933 - val_loss: 0.7362 - val_binary_accuracy: 0.3015\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7371 - binary_accuracy: 0.2934 - val_loss: 0.7361 - val_binary_accuracy: 0.3015\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7370 - binary_accuracy: 0.2937 - val_loss: 0.7361 - val_binary_accuracy: 0.3022\n"
     ]
    }
   ],
   "source": [
    "model_ANN = load_model('./Models/model_ANN_init.h5')\n",
    "\n",
    "history = model_ANN.fit(x_train, y_train,\n",
    "                            batch_size = batch_size,\n",
    "                            epochs = epochs,\n",
    "                            validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ANN.save('./Models/model_ANN_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost classifier\n",
    "\n",
    "To not only test neural networks, we here test another standard machine learning algorithm. XGBoost has become a method of choice in recent Kaggle competitions (althought this does not mean that it will perform better than neural networks in the present case - \"No Free Lunch\" theorem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [5, 7, 10]\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(estimator = xgb.XGBClassifier(\n",
    "                          objective = \"binary:logistic\"\n",
    "                          ),\n",
    "                        param_grid = params,\n",
    "                        scoring='accuracy',\n",
    "                        n_jobs=1,\n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             n_jobs=1, param_grid={'max_depth': [5, 7, 10]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([3.24635272, 4.41634746, 5.22779336]),\n",
       "  'std_fit_time': array([0.32545536, 0.85535216, 0.15493848]),\n",
       "  'mean_score_time': array([0.02274785, 0.02811713, 0.03941512]),\n",
       "  'std_score_time': array([0.00126596, 0.00088646, 0.0050415 ]),\n",
       "  'param_max_depth': masked_array(data=[5, 7, 10],\n",
       "               mask=[False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'max_depth': 5}, {'max_depth': 7}, {'max_depth': 10}],\n",
       "  'split0_test_score': array([0.78036972, 0.77985622, 0.77633509]),\n",
       "  'split1_test_score': array([0.78572477, 0.78359742, 0.77882923]),\n",
       "  'split2_test_score': array([0.79245837, 0.78857017, 0.78314137]),\n",
       "  'split3_test_score': array([0.78409508, 0.7848287 , 0.77917981]),\n",
       "  'split4_test_score': array([0.77932654, 0.77807938, 0.77653877]),\n",
       "  'mean_test_score': array([0.78439489, 0.78298638, 0.77880485]),\n",
       "  'std_test_score': array([0.00466408, 0.00371006, 0.00245688]),\n",
       "  'rank_test_score': array([1, 2, 3], dtype=int32)},\n",
       " {'max_depth': 5},\n",
       " 0.7843948926053324)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.cv_results_, gridsearch.best_params_, gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Models/model_XGBoost.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(gridsearch, './Models/model_XGBoost.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
